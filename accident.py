# -*- coding: utf-8 -*-
"""Accident.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DPCdMsZXs2PqRp1l3DZqcGL1e48OqZXd
"""

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
# Specify the file path
#file_path = '/content/drive/MyDrive/kumkum/AccidentDataTestv3.csv'  # Replace with your file name or path
file_path = '/content/Resampled_AccidentData.csv'  # Replace with your file name or path
font = {'family' : 'sans-serif',
        'weight' : 'normal',
        'size'   : 22}

matplotlib.rc('font', **font)
# Read the Excel file
df = pd.read_csv(file_path)
X=df['Year']
Y=df['Severity_Grade']
print(Y.shape)

# Optional: Visualize Grade Distribution by Year
grade_counts = df.groupby(['Year', 'Severity_Grade']).size().reset_index(name='Count')
grade_pivot = grade_counts.pivot(index='Year', columns='Severity_Grade', values='Count').fillna(0)

# Plot
grade_pivot.plot(kind='bar', stacked=True, figsize=(15, 6), colormap='Accent')
plt.title('Year-wise Grade Distribution')
plt.xlabel('Year',fontsize=20,)
plt.ylabel('Count',fontsize=20)
plt.xticks(rotation=0)
plt.legend(title='Grade')
plt.savefig('/content/drive/MyDrive/kumkum/grade_distribution.pdf',dpi=1000)
plt.show()

import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
# Specify the file path
file_path = '/content/drive/MyDrive/kumkum/AccidentDataTestv3.csv'  # Replace with your file name or path
#file_path = '/content/Resampled_AccidentData.csv'  # Replace with your file name or path
font = {'family' : 'sans-serif',
        'weight' : 'normal',
        'size'   : 22}

matplotlib.rc('font', **font)
# Read the Excel file
df = pd.read_csv(file_path)
X=df['Year']
Y=df['Severity_Grade']
print(Y.shape)

# Optional: Visualize Grade Distribution by Year
grade_counts = df.groupby(['Year', 'Severity_Grade']).size().reset_index(name='Count')
grade_pivot = grade_counts.pivot(index='Year', columns='Severity_Grade', values='Count').fillna(0)

# Plot
grade_pivot.plot(kind='bar', stacked=True, figsize=(15, 6), colormap='Accent')
plt.title('Year-wise Grade Distribution')
plt.xlabel('Year',fontsize=20)
plt.ylabel('Count',fontsize=20)
plt.xticks(rotation=0)
plt.legend(title='Grade')
#plt.savefig('/content/drive/MyDrive/kumkum/grade_distribution.pdf',dpi=1000)
plt.show()

# Install required library
# !pip install imblearn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

# File path
file_path = '/content/drive/MyDrive/kumkum/AccidentDataTestv3.csv'

# Load dataset
df = pd.read_csv(file_path)

# Verify column names
print("Columns in dataset:", df.columns)

# Drop irrelevant columns (ensure they exist)
columns_to_drop = ['severity', 'severity_norm']
df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], axis=1)

# Split features and target
if 'Severity_Grade' in df.columns:
    X = df.drop(['Severity_Grade'], axis=1)
    y = df['Severity_Grade']
else:
    raise KeyError("Severity_Grade column not found in the dataset")

# Encode categorical target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Check class distribution
print("Class distribution before splitting:\n", pd.Series(y_encoded).value_counts())

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=0)

# Check class distribution in training set
print("Before OverSampling, counts of each class:\n", pd.Series(y_train).value_counts())

# Apply SMOTE for oversampling
# Use `sampling_strategy='auto'` to balance all classes equally
sm = SMOTE(sampling_strategy={1: 150,2:150,0:150}, random_state=2)  # Adjust strategy if needed
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Check new shape and distribution
print('After OverSampling, the shape of train_X:', X_train_res.shape)
print("After OverSampling, counts of each class:\n", pd.Series(y_train_res).value_counts())

# Decoding labels back to original form if necessary
y_train_res_decoded = le.inverse_transform(y_train_res)

# Save the resampled data if needed
resampled_df = pd.DataFrame(X_train_res, columns=X.columns)
resampled_df['Severity_Grade'] = y_train_res_decoded
resampled_df.to_csv('/content/drive/MyDrive/kumkum/Resampled_AccidentData.csv', index=False)

print("Resampled dataset saved as 'Resampled_AccidentData.csv'")

#correlation heat map

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/kumkum/Resampled_AccidentData.csv')

font = {'family' : 'sans-serif',
        'weight' : 'normal',
        'size'   : 19}
color=sns.color_palette("Set2", as_cmap=True)
matplotlib.rc('font', **font)
# Split features and target
df = df.drop(['Total'], axis=1)
print(df.columns)


# Encode categorical target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Generate correlation matrix
correlation_matrix = df.corr()

# Plot the correlation heatmap
plt.figure(figsize=(15, 8))
sns.heatmap(correlation_matrix, annot=True, cmap=color, fmt='.2f', cbar=True)
plt.title('Correlation Heatmap of Features')
plt.savefig('/content/drive/MyDrive/kumkum/correlation_heatmap.pdf',dpi=1000,bbox_inches='tight')
plt.show()

!pip install xgboost

#Grid search
import pandas as pd
import matplotlib
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
# Load the dataset
from sklearn.model_selection import KFold,GridSearchCV, cross_val_score,StratifiedKFold
df = pd.read_csv('/content/drive/MyDrive/kumkum/Resampled_AccidentData.csv')

X = df.drop(['Severity_Grade','Total'], axis=1)
y = df['Severity_Grade']

# Encode categorical target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Normalize the features
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)

font = {'family' : 'sans-serif',
        'weight' : 'normal',
        'size'   : 19}

matplotlib.rc('font', **font)

#X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=0)

# Encode categorical target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

#Applying K fold cross validation
kfold = KFold(n_splits=5, shuffle=False)


# param_grid = {
#     'hidden_layer_sizes': [ 50, 100, 100,150,200,300,400],
#     'activation': ['tanh', 'relu'],#removed 'auto' parameter from here
#     'alpha' : [0.01,0.1,0.5,0.05,0.001,0.005],

#     }

# param_grid = {
#     'n_estimators': [50, 60, 70, 100,150,200],
#     'max_features': [None, 'sqrt', 'log2'],#removed 'auto' parameter from here
#     'max_depth' : [3,4,5,7,9,10],
#     'min_samples_leaf': [2,3,5,7],

#     }

param_grid = {
    'eta': [0.1, 0.2, 0.3, 0.4,0.5],
    'min_child_weight': [2,3,5,7],#removed 'auto' parameter from here
    'max_depth' : [3,4,5,7,9],

    }
##--------------------------------------------------------------------------##

#clf=RandomForestClassifier(random_state=42)
#clf = ExtraTreesClassifier(random_state=42)
clf = XGBClassifier(random_state=42)
#clf = DecisionTreeClassifier(random_state=42)
#clf = MLPClassifier(random_state=42, max_iter=800,early_stopping=True)
#clf = MLPRegressor(random_state=1, max_iter=800,early_stopping=True
#clf=ExtraTreesRegressor()
#clf = MLPRegressor(random_state=1, max_iter=800,early_stopping=True)
CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 5)
grid_result=CV_rfc.fit(X_normalized, y)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
df = pd.read_csv('/content/drive/MyDrive/kumkum/Resampled_AccidentData.csv')
X = df.drop(['Severity_Grade','Total'], axis=1)
y = df['Severity_Grade']

# Encode categorical target
le = LabelEncoder()
y_encoded = le.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)
print(X_train.shape)

# Create an SVM classifier
#rf_clf = SVC(kernel='linear', C=1, random_state=42)  # Using a linear kernel
#rf_clf = SVC(kernel='poly', C=1, random_state=42)

rf_clf = LogisticRegression(multi_class= 'ovr')
#rf_clf = RandomForestClassifier(random_state=42, n_estimators=150, max_depth=9,max_features='sqrt',min_samples_leaf=2)
#rf_clf = ExtraTreesClassifier(random_state=42, n_estimators=60, max_depth=9,max_features=None,min_samples_leaf=2)
#rf_clf = XGBClassifier(random_state=42, eta=0.4, max_depth=3,min_child_weight =3)
#rf_clf = MLPClassifier(random_state=42, max_iter=800,early_stopping=True,activation='tanh',alpha=0.1,hidden_layer_sizes=300)
#rf_clf = DecisionTreeClassifier(random_state=42, max_depth=7,max_features=None,min_samples_leaf=3)
rf_clf.fit(X_train, y_train)


# Predict labels for test set
y_truth = rf_clf.predict(X_train)
# Predict labels for test set
y_pred = rf_clf.predict(X_test)
print(y_pred.shape)
# Evaluate the model
accuracy_true = accuracy_score(y_truth, y_train)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
class_report_train = classification_report(y_truth, y_train)

# Display metrics
print("Accuracy of Training Random Forest Classifier:", accuracy_true)
print("Accuracy of Random Forest Classifier:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)
print("\nClassification Report_train:\n", class_report_train)

# Define colors and markers for each class
class_colors = ['r', 'g', 'b']  # Red, Green, Blue for the three classes
class_markers = ['^', 'o', 's']  # Triangle, Circle, Square for ground truth and predictions
class_labels = ['Class 0', 'Class 1', 'Class 2']


# Create the plot
fig, ax1 = plt.subplots(figsize=(17, 7), dpi=900)

# Keep track of added labels for the legend
added_labels = set()

# Plot ground truth and predictions for each sample
for i in range(len(y_test)):
    true_class = int(y_test[i])  # Ground truth class
    pred_class = int(y_pred[i])  # Predicted class

    # Line connecting ground truth and prediction
    ax1.plot([i, i], [y_test[i], y_pred[i]], c="k", linewidth=0.6, alpha=0.6)

     # Plot ground truth
    gt_label = f'Ground Truth {class_labels[true_class]}'
    if gt_label not in added_labels:
        ax1.scatter(i, y_test[i], color=class_colors[true_class], marker=class_markers[0],s=40, label=gt_label)
        added_labels.add(gt_label)
    else:
        ax1.scatter(i, y_test[i], color=class_colors[true_class], marker=class_markers[0],s=40)

    # Plot prediction
    pred_label = f'Prediction {class_labels[pred_class]}'
    if pred_label not in added_labels:
        ax1.scatter(i, y_pred[i], color=class_colors[pred_class], marker=class_markers[1],s=40,
                    label=pred_label)
        added_labels.add(pred_label)
    else:
        ax1.scatter(i, y_pred[i], color=class_colors[pred_class], marker=class_markers[1],s=40)

# Add Y-axis labels for classes
ax1.set_yticks([0, 1, 2])  # Assuming classes are 0, 1, 2
ax1.set_yticklabels(class_labels)

# Add labels, title, legend, and grid
ax1.set_xlabel('Sample Index')
ax1.set_ylabel('Severity Grade')
ax1.set_title('Ground Truth vs Prediction for Each Sample (Three Classes)')
ax1.legend(loc='upper left', fontsize=14, bbox_to_anchor=(0.8, 0.9))
#ax1.grid(True, linestyle='--', alpha=0.7)

# Adjust layout and show the plot
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/kumkum/LR_test_plot.pdf',dpi=1000)
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import matplotlib

# Data for the plot
models = ['RF', 'ET', 'XGB', 'DT', 'MLP', 'SVM', 'LR']
training_acc = [0.9968, 0.9968, 0.9968, 0.968, 0.917, 0.984, 0.98]
test_acc = [0.977, 1, 0.985, 0.918, 0.881, 0.962, 0.96]

font = {'family' : 'sans-serif',
        'weight' : 'normal',
        'size'   : 13}

matplotlib.rc('font', **font)
# Plot configurations
x = np.arange(len(models))  # positions for the groups
width = 0.42  # width of the bars

# Create the bar plot
fig, ax = plt.subplots(figsize=(10, 6))
bar1 = ax.bar(x - width/2, [val * 0.8 for val in training_acc], width, label='Training Accuracy', color='skyblue')
bar2 = ax.bar(x + width/2, [val * 0.8 for val in test_acc], width, label='Test Accuracy', color='orange')

# Add labels, title, and legend
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
ax.set_title('Training and Test Accuracy for Different Models')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.set_ylim(0.5, 1.0)  # Set Y-axis range
ax.legend()

# Annotate the bars with values
for bar in bar1 + bar2:
    height = bar.get_height()
    ax.annotate(f'{height / 0.8:.3f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom')

# Show the plot
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/kumkum/accuracy_plot.pdf',dpi=1000)
plt.show()

#resampled and augmented data
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

df = pd.read_csv('/content/drive/MyDrive/kumkum/AccidentDataTestv3.csv')
X = df.drop(['Severity_Grade'], axis=1)
y = df['Severity_Grade']

# Encode categorical target
le = LabelEncoder()
y_encoded = le.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=0)

#rf_clf = RandomForestClassifier(random_state=42, n_estimators=60, max_depth=9,max_features='sqrt',min_samples_leaf=2)
rf_clf = ExtraTreesClassifier(random_state=42, n_estimators=60, max_depth=9,max_features=None,min_samples_leaf=2)
rf_clf.fit(X_train, y_train)

# Predict labels for test set
y_pred = rf_clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Display metrics
print("Accuracy of Random Forest Classifier:", accuracy)
print("\nConfusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

import shap
import pandas as pd
import matplotlib.pyplot as plt
#from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load data
df = pd.read_csv('/content/drive/MyDrive/kumkum/Resampled_AccidentData.csv')

# Prepare features (X) and single-column target (y)
X = df.drop(['Severity_Grade', 'Total'], axis=1)
y = df['Severity_Grade']  # Single target column with 3 classes

# Encode categorical target (if needed)
le = LabelEncoder()

y_encoded = le.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=0)

# Train a Random Forest model
rf_clf = ExtraTreesClassifier(random_state=42, n_estimators=60, max_depth=9,max_features=None,min_samples_leaf=2)
#rf_clf = RandomForestClassifier(random_state=42, n_estimators=60, max_depth=9,max_features='sqrt', min_samples_leaf=2)
rf_clf.fit(X_train, y_train)

# Explain model predictions using SHAP
explainer = shap.Explainer(rf_clf, X_train)
shap_values = explainer(X_test)

# Loop through classes to plot and save SHAP summary for each class
for class_idx in range(len(le.classes_)):
    print(class_idx)
    class_name = le.classes_[class_idx]  # Get original class name
    print(f"Generating and saving SHAP summary plot for class: {class_name}")

    # Set figure size
    plt.figure(figsize=(12, 8))

    # Generate SHAP summary plot for the specific class
    shap.summary_plot(shap_values[:, :, class_idx], X_test, feature_names=X.columns, show=False)

    # Save the plot to a file
    file_name = f"shap_summary_class_{class_name}.png"
    plt.savefig(file_name, bbox_inches='tight',dpi=900)  # Save with tight bounding box

    # Close the plot to avoid overlap in subsequent plots
    plt.show()

print("SHAP summary plots saved successfully.")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.inspection import PartialDependenceDisplay

# Load data
df = pd.read_csv('/content/drive/MyDrive/kumkum/AccidentDataTestv3.csv')

# Prepare features (X) and target (y)
X = df.drop(['Severity_Grade', 'Total'], axis=1)
y = df['Severity_Grade']  # Single target column with multiple classes

# Encode categorical target (if needed)
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=0)

# Train a Random Forest model
rf_clf = RandomForestClassifier(random_state=42, n_estimators=60, max_depth=9,
                                 max_features='sqrt', min_samples_leaf=2)
rf_clf.fit(X_train, y_train)

# Generate Partial Dependence Plots (PDP) for specific features
features_to_plot = ['Fatal', 'No Injury']  # Replace with actual feature names in X

# Specify the class index for the PDP
class_index = 2  # You can adjust this to another class index if needed

fig, axes = plt.subplots(ncols=2, figsize=(10, 5))
# Loop through features and generate PDP for each
for i, feature in enumerate(features_to_plot):
    if feature in X.columns:  # Ensure the feature exists in X
        print(f"Generating PDP for feature: {feature}, class index: {class_index}")

        # Ensure the index does not exceed the number of axes available
        ax = axes[i % 2]  # Ensure that axes is reused if more than 2 features

        # Generate the partial dependence plot
        PartialDependenceDisplay.from_estimator(
            rf_clf,
            X_train,
            [feature],
            kind='average',
            target=class_index,  # Specify the class index
            grid_resolution=100,
            ax=ax
        )

# Adjust layout to avoid overlap
plt.tight_layout()
plt.savefig("PDP_class2.pdf",dpi=900,bbox_inches='tight')
plt.show()